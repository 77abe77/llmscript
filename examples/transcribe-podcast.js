// Transcribe and combine multiple channels of a podcast into
// a single timestamped text file with speaker names.
// Code Generated by an llm: https://chat.openai.com/c/ce61096c-4a89-4656-a886-2add514dd940
// Audio splitting: ffmpeg -i Test.mp3 -f segment -segment_time 650 -reset_timestamps 1 -strftime 1 ../Test-%Y%m%d-%H%M%S-%03d.mp3

import { OpenAI, OpenAIFastOptions } from 'llmclient';
import { PromisePool } from '@supercharge/promise-pool';
import path from 'path';
import fs from 'fs';

// Initialize OpenAI
const ai = new OpenAI(process.env.OPENAI_APIKEY, OpenAIFastOptions());
const args = process.argv.slice(2);
const audioPath = args[0];
const outputFolder = path.join(audioPath, 'output');

if (!fs.existsSync(outputFolder)) {
  fs.mkdirSync(outputFolder);
}

const audioFiles = fs
  .readdirSync(args[0])
  .filter((v) => v.endsWith('.mp3'))
  .map((v) => path.join(audioPath, v));

// Function to extract the prefix from filename
const getPrefix = (file) => {
  const match = path.parse(file).name.match(/^([^-]+)/);
  return match ? match[0] : 'unknown';
};

// Function to extract the timestamp from filename
const getTimestamp = (file) => {
  const match = path.parse(file).name.match(/\d{8}-\d{6}/);
  return match ? new Date(match[0]) : new Date();
};

// Group audio files by prefix
const groupedAudioFiles = audioFiles.reduce((acc, file) => {
  const prefix = getPrefix(file);
  if (!acc[prefix]) {
    acc[prefix] = [];
  }
  acc[prefix].push(file);
  return acc;
}, {});

const toTime = (sec) => new Date(sec * 1000).toISOString().slice(11, 19);

// Function to handle the transcription task
const transcribe = async (v) => {
  let segments;
  try {
    ({ segments } = await ai.transcribe(v));
  } catch (e) {
    console.error(`Error transcribing ${v}: ${e.message}`);
    return [];
  }

  // Extract the timestamp from the filename
  const timestamp = getTimestamp(v);

  return segments.map(({ start, end, text }) => {
    // Add the timestamp to the start and end times
    const adjustedStart = new Date(timestamp.getTime() + start * 1000);
    const adjustedEnd = new Date(timestamp.getTime() + end * 1000);
    return {
      start: toTime(adjustedStart / 1000),
      end: toTime(adjustedEnd / 1000),
      text,
    };
  });
};

let finalOutput = []; // To keep track of all transcriptions

for (let prefix in groupedAudioFiles) {
  const files = groupedAudioFiles[prefix];
  const outputPath = path.join(outputFolder, `${prefix}.json`);
  const combinedTranscriptPath = path.join(
    outputFolder,
    `${prefix}_transcript.txt`
  );

  // Check if transcripts already exist
  if (fs.existsSync(outputPath) && fs.existsSync(combinedTranscriptPath)) {
    console.log(`Skipping ${prefix}, transcripts already exist.`);
    continue;
  }

  // Start uploading audio files for transcription 2 at a time in parallel
  const { results } = await PromisePool.withConcurrency(2)
    .for(files)
    .onTaskFinished((_, pool) => console.log(pool.processedPercentage()))
    .process(transcribe);

  const output = results
    .flat()
    .sort((a, b) => new Date(a.start) - new Date(b.start)) // compare start time date objects
    .map((v) => `${v.start}-${v.end}: ${v.text}`);

  // Save individual transcriptions and combined transcription
  fs.writeFileSync(outputPath, JSON.stringify(results));
  fs.writeFileSync(combinedTranscriptPath, output.join('\n'));

  // Add to the final output
  finalOutput = [...finalOutput, ...output];
}

// Sort the final output and write it to a file
finalOutput = finalOutput
  .sort((a, b) => new Date(a.split('-')[0]) - new Date(b.split('-')[0])) // get the start time from the string and convert to date for comparison
  .join('\n');

fs.writeFileSync(path.join(outputFolder, 'final_transcript.txt'), finalOutput);
